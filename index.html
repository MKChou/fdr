<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MK love you forever</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Real-Time Multimodal Fall Detection via Audio-Visual Fusion and DST on Edge Devices</h1>
    <p> MK Chou</p>
  </header>

  <nav>
    <ul>
      <li><a href="#overview">Overview</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#dst-reason">DST</a></li>
      <li><a href="#system-architecture">System Architecture</a></li>
      <li><a href="#visual-component">Visual</a></li>
      <li><a href="#audio-component">Audio</a></li>
      <li><a href="#fusion-decision">Fusion</a></li>
      <li><a href="#cli-dashboard">CLI</a></li>
      <li><a href="#performance">Performance</a></li>
      <li><a href="#contact">Contact</a></li>
    </ul>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      FallFusion is a real-time multimodal fall detection system that integrates visual and audio sensing data. It employs a self-trained CNN model for action classification and an LSTM model for fall sound recognition, applying Dempster-Shafer Theory for decision-level fusion. The system has been successfully deployed on the resource-constrained Jetson Nano 2GB platform, demonstrating efficient memory usage, ONNX model cross-platform integration, and GPIO hardware control capabilities, thereby enhancing overall stability and application feasibility.
    </p>
  </section>

  <section id="image-section" style="text-align:center;">
    <h2 style="text-align:left;">Links</h2>
    <div class="qrcode-list">
      <div class="qrcode-row">
        <img src="Q1.png" alt="Q1 image" class="qrcode-img">
        <span class="qrcode-label">DST</span>
        <a href="https://github.com/MKChou/DST-FallNet" target="_blank" style="margin-left:12px;">GitHub Repo 1</a>
      </div>
      <div class="qrcode-row">
        <img src="Q2.png" alt="Q2 image" class="qrcode-img">
        <span class="qrcode-label">CNN</span>
        <a href="https://github.com/MKChou/fall-detection-cnn" target="_blank" style="margin-left:12px;">GitHub Repo 2</a>
      </div>
      <div class="qrcode-row">
        <img src="Q3.png" alt="Q3 image" class="qrcode-img">
        <span class="qrcode-label">LSTM</span>
        <a href="https://github.com/MKChou/fall-sound-detection-lstm" target="_blank" style="margin-left:12px;">GitHub Repo 3</a>
      </div>
    </div>
  </section>

  <section id="motivation">
    <h2>Motivation and Challenges</h2>
    <p>With the advent of an aging society, falls have become one of the leading causes of injury and death among the elderly. Traditional detection systems often rely on a single modality (such as cameras or microphones), but:</p>
    <ul>
      <li>Single-modality accuracy is limited: Visual data is affected by lighting and occlusion, while audio is susceptible to background noise, leading to frequent false alarms or missed detections.</li>
      <li>High resource requirements: Most deep learning systems rely on high-performance computers and cloud services, making them unsuitable for resource-constrained environments.</li>
      <li>Privacy and latency issues: Uploading images to the cloud for analysis raises privacy concerns and cannot provide real-time responses.</li>
    </ul>
    <p>Therefore, we aim to design a system that can:</p>
    <ul>
      <li>Integrate multiple sensing data (visual + audio) to improve recognition accuracy.</li>
      <li>Deploy on low-memory, low-power embedded platforms such as Jetson Nano 2GB.</li>
      <li>Support real-time response and hardware interaction (LED, buzzer) to enhance alert efficiency and practical value.</li>
    </ul>
  </section>

  <section id="dst-reason">
    <h2>Why Dempster-Shafer Theory (DST)?</h2>
    <p>Traditional multimodal fusion methods (such as direct score averaging) lack flexibility in handling uncertainty and conflicts. For example:</p>
    <ul>
      <li>The visual model may have low confidence under poor lighting.</li>
      <li>The audio model may misclassify due to background noise.</li>
    </ul>
    <p>Directly combining scores in such cases may reduce system accuracy.</p>
    <p><b>How DST addresses these issues:</b></p>
    <ul>
      <li><b>Handling uncertainty:</b><br>DST can assign belief to "uncertainty" itself (e.g., m(unknown)), offering more flexibility than traditional probability methods.</li>
      <li><b>Fusing conflicting information:</b><br>When the visual and audio models disagree (e.g., one detects a fall, the other normal), DST calculates a "conflict coefficient K" and reallocates trust to the more reliable modality.</li>
      <li><b>Dynamic weight adjustment:</b><br>No need to manually set the weight of visual or audio; DST automatically determines which is more trustworthy based on the current input.</li>
    </ul>
  </section>

  <section id="system-architecture">
    <h2>System Architecture</h2>
    <img src="system.png" alt="System Architecture Diagram" style="max-width:90%;height:auto;display:block;margin:0 auto;">
    <p style="text-align:center;">System architecture flowchart</p>
    <p>This system combines vision and sound to detect falls more robustly, all while running on a low-resource embedded device. The core components include:</p>
    <ul>
      <li>Pose estimation (MoveNet)</li>
      <li>Audio recognition (MFCC + LSTM)</li>
      <li>Fusion via Dempster-Shafer Theory (DST)</li>
      <li>Confirmation using a self-trained 5-class CNN</li>
      <li>Deployment on Jetson Nano 2GB using ONNX inference</li>
    </ul>
    <h3 style="text-align:left;">The system consists of four main threads:</h3>
    <ol style="text-align:left;">
      <li><b>Visual Pose Inference Thread</b><br>
        Captures camera images, resizes and preprocesses them for input to the MoveNet ONNX model.<br>
        Infers 17 human keypoints and confidence scores.<br>
        Calculates body tilt angle to estimate suspicious postures (e.g., lying, falling).
      </li>
      <li><b>Audio Recognition Thread</b><br>
        Continuously records audio (3 seconds) via microphone.<br>
        Converts audio to MFCC (Mel Frequency Cepstral Coefficients) feature maps.<br>
        Inputs to LSTM audio model (ONNX), outputs a fall sound score (0~1).
      </li>
      <li><b>Fusion & CNN Verification Thread</b><br>
        Fuses visual and audio inference scores using DST (Dempster-Shafer Theory).<br>
        If the fusion result exceeds the threshold, captures the current image and uses a 5-class CNN model (standing, sitting, lying, bending, crawling) for final confirmation.<br>
        Updates results to the CLI Dashboard and decides whether to trigger GPIO output.
      </li>
      <li><b>GPIO Control & Button Monitoring Thread</b><br>
        Controls external hardware outputs such as LED/buzzer.<br>
        Detects button triggers, clears fall event records, and resets system status.
      </li>
    </ol>
  </section>

  <section id="visual-component">
    <h2>Visual Component</h2>
    <h3>1. MoveNet Pose Estimation</h3>
    <p>MoveNet is a high-performance human pose estimation model developed by Google, featuring extremely fast inference speed and high accuracy. Its lightweight design is ideal for deployment on embedded devices such as Jetson Nano, enabling stable and accurate pose analysis even under resource constraints. With MoveNet, the system can instantly capture human motion changes, providing a solid foundation for subsequent fall detection and multimodal fusion.</p>
    <img src="movenet.gif" alt="MoveNet pose estimation demo" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">MoveNet real-time keypoint detection demo</p>
    <img src="movenet2.gif" alt="MoveNet fall detection process demo" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">MoveNet applied to fall detection process animation</p>
    <p>The system applies the MoveNet ONNX model for human pose estimation, detecting 17 keypoints (e.g., head, shoulders, elbows, knees) and outputting coordinates and confidence scores. These features are used to calculate body tilt angle and center of gravity, serving as the initial basis for fall detection. The process ensures high performance and low latency, suitable for embedded platforms.</p>
    <p>To further optimize deployment, MoveNet is quantized to int8 and converted from tflite to onnx format. This significantly reduces model size and computational requirements with minimal impact on inference accuracy, enabling stable operation on resource-limited devices like Jetson Nano while maintaining real-time and precision.</p>
    <h3>2. Custom CNN Action Classification Verification</h3>
    <p>When the DST fusion score exceeds the preset threshold, a custom-trained CNN model is activated for further action classification. This CNN is specifically designed for the system and can classify human actions into five categories (standing, sitting, lying, bending, crawling), providing a confidence score for each. If the confidence for any action exceeds the alert threshold, the system triggers an alarm. This design effectively reduces false alarms and improves the accuracy and reliability of fall detection.</p>
    <p>The training dataset used is from <a href="https://falldataset.com/" target="_blank">Fall Detection Dataset</a>. Each folder contains sequential action images with a CSV file labeling the action type for each image. A total of 15,402 images were processed, split into 80% training and 20% validation sets. During training, both training and validation accuracy and loss are displayed in real time to monitor model learning and generalization.</p>
    <img src="CNN_confusion_matrix.png" alt="CNN confusion matrix" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">CNN action classification confusion matrix, showing prediction vs. actual distribution</p>
    <img src="CNN_acc_curve.png" alt="CNN accuracy curve" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">CNN training and validation accuracy curves</p>
  </section>

  <section id="audio-component">
    <h2>Audio Component</h2>
    <h3>1. Audio Data Collection and Preprocessing</h3>
    <p>The audio module continuously collects environmental sounds using a microphone, with each recording lasting 3 seconds. After recording, noise reduction and volume normalization are performed, and the audio waveform is converted into MFCC (Mel Frequency Cepstral Coefficient) features for model input. MFCC effectively represents the time-frequency characteristics of sound and is a common feature representation in speech and event recognition.</p>
    <img src="Audio_wave.png" alt="Audio waveform" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">Original audio waveform</p>
    <img src="Audio_MFCC.png" alt="Audio MFCC feature" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">Audio waveform converted to MFCC feature for model input</p>

    <h3>2. Dataset Source</h3>
    <p>The audio model is trained using the <a href="https://www.kaggle.com/datasets/antonygarciag/fall-audio-detection-dataset/data" target="_blank">AFE: Sound Analysis for Fall Event detection</a> dataset, which contains various fall events and daily environmental sounds, with each audio segment labeled as fall-related or not. These annotations help the model learn to distinguish fall sounds from general noise, improving recognition accuracy.</p>

    <h3>3. LSTM Model Training and Performance</h3>
    <ul>
      <li><b>Model architecture:</b> LSTM(64) → Dropout(0.5) → Dense(32, relu) → Dense(1, sigmoid)</li>
      <li><b>Loss function:</b> binary_crossentropy</li>
      <li><b>Optimizer:</b> Adam</li>
      <li><b>Epochs:</b> 30</li>
      <li><b>Batch size:</b> 16</li>
      <li><b>Train/Test Split:</b> 80/20</li>
    </ul>
    <p>MFCC features are used as input to train the LSTM model for fall sound recognition. The following figures show the LSTM training loss/accuracy curves, test set confusion matrix, and ROC curve, demonstrating the model's excellent performance in classifying fall and non-fall events.</p>
    <img src="Audio_LSTM_training_results.png" alt="LSTM training loss/accuracy" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM training loss and accuracy curves</p>
    <img src="Audio_Confusion_Matrix.png" alt="LSTM confusion matrix" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM model confusion matrix on the test set</p>
    <img src="Audio_ROCCurve.png" alt="LSTM ROC curve" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM model ROC curve, AUC indicates excellent classification ability</p>

    <h3>4. Advanced Evaluation Metrics</h3>
    <ul>
      <li><b>Confusion Matrix:</b> [[91, 2], [3, 94]]</li>
      <li><b>Accuracy:</b> 0.97</li>
      <li><b>Precision:</b> 0.9792</li>
      <li><b>Recall:</b> 0.9691</li>
      <li><b>F1-score:</b> 0.9741</li>
      <li><b>AUC Score:</b> 0.9973</li>
      <li><b>ROC curve:</b> Nearly perfect (see above)</li>
    </ul>

    <h3>5. Practical Application Plan</h3>
    <ul>
      <li>Real-time audio collection with recording equipment</li>
      <li>Convert audio to MFCC (40 dimensions, 3 seconds, 22050Hz)</li>
      <li>Input to the trained model (.tflite/.onnx) to determine if a fall event has occurred</li>
      <li>Output a "fall probability score" for use in multimodal fusion (combined with video, accelerometer, etc.)</li>
    </ul>
  </section>

  <section id="fusion-decision">
    <h2>Fusion & Decision</h2>
    <h3>1. Fusion Mechanism</h3>
    <p>This system adopts Dempster-Shafer Theory (DST) as the core of multimodal decision fusion. First, inference results from the visual module (e.g., pose angle, CNN score) and the audio module (LSTM score) are obtained and converted into belief assignments. The DST fusion algorithm then calculates the final fused confidence score based on the individual confidence values, while also producing a conflict coefficient (K) that reflects the degree of disagreement between the two modalities.</p>
    <p>Specifically, this system utilizes a late fusion (decision-level fusion) strategy. Instead of directly combining raw sensor data or features, each modality (vision and audio) independently produces its own decision or confidence score. These scores are then fused at the decision level using DST. This approach allows each model to fully leverage its strengths and makes the overall system more robust to noise or errors in any single modality. Late fusion is especially suitable for scenarios where sensor characteristics and data types differ significantly, as in this fall detection application.</p>
    <p>The system is also designed to flexibly support different sensors and threshold settings for various environments. Depending on the deployment scenario, users can select the most appropriate combination of sensors (e.g., only vision, only audio, or both) and adjust the fusion or alert thresholds accordingly. This adaptability ensures optimal performance and reliability whether in a quiet home, a noisy public space, or other specialized settings.</p>

    <h3>2. Threshold & Trigger</h3>
    <p>When the DST fused confidence score exceeds a preset threshold, the system further activates the CNN model for multi-class action verification. If the CNN determines a high-risk action (such as lying or crawling) and the confidence exceeds the alert threshold, an alert signal (e.g., buzzer, LED) is triggered and the event is logged in the CLI Dashboard.</p>

    <h3>3. Conflict Handling</h3>
    <p>One major advantage of DST is its ability to dynamically handle conflicts between modalities. For example, when the visual and audio modules produce opposite results, DST calculates the conflict coefficient K and automatically adjusts the weights, assigning trust to the more reliable source. This prevents system failure due to misjudgment by a single modality.</p>

    <h3>4. Practical Value</h3>
    <p>DST-based fusion and decision-making significantly enhance the system's robustness and accuracy. Even when a single sensor is disturbed (e.g., poor lighting, environmental noise), the system can still make stable judgments based on multimodal information, reducing false alarms and missed detections. This makes it suitable for long-term automated monitoring applications.</p>

  
  </section>

  <section id="cli-dashboard">
    <h2>CLI Dashboard & Monitoring</h2>
    <h3>1. Interface Design</h3>
    <p>The CLI Dashboard adopts a pure text-based interface, presenting information in clear tables and blocks, with color coding (e.g., red for alerts, green for normal) to enhance instant recognition. The interface is concise and easy to read, suitable for embedded devices and remote terminal operation.</p>

    <h3>2. Real-Time Monitoring Features</h3>
    <p>The CLI interface displays multiple key pieces of information in real time, including:
      <ul>
        <li>System runtime status (memory, CPU usage, latency, etc.)</li>
        <li>MoveNet pose keypoint coordinates and confidence values</li>
        <li>Audio MFCC features and LSTM scores</li>
        <li>DST fusion score and conflict coefficient</li>
        <li>CNN multi-class results and confidence values</li>
        <li>Current alert status (e.g., fall detected, alert triggered)</li>
      </ul>
    </p>

    <h3>3. Event Log and Alert Confirmation</h3>
    <p>The lower part of the interface records all fall events with time, confidence score, and action type for easy query and tracking. When an alert is triggered, it is clearly marked, and users can confirm or reset events directly via the CLI.</p>

    <h3>4. Cross-Platform and Lightweight</h3>
    <p>The CLI Dashboard runs on various operating systems and supports remote SSH connections, making it ideal for embedded platforms (such as Jetson Nano) or server-side deployment. Its lightweight nature ensures smooth operation even in resource-constrained environments.</p>

    <h3>5. Practical Value</h3>
    <p>The CLI interface enables on-site personnel or remote maintenance engineers to instantly grasp system status, quickly respond to abnormal events, and serves as a foundation for long-term monitoring and data analysis. It is especially valuable in environments where graphical interfaces are unavailable (e.g., factories, edge devices).</p>

    <img src="CLI.png" alt="CLI Dashboard interface" style="max-width:90%;height:auto;display:block;margin:0 auto;">
    <p style="text-align:center;">CLI Dashboard monitoring interface demo</p>
  </section>

  <section id="performance">
    <h2>Model Performance</h2>
    <p>The following table summarizes the key specifications, runtime behavior, and accuracy of the multimodal system, including CNN, LSTM, and DST fusion. This unified view allows for easy comparison of all core metrics.</p>
    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Input Shape</th>
          <th>Inference Latency</th>
          <th>Accuracy</th>
          <th>DST Fusion Latency</th>
          <th>Status</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>CNN (5-class)</td>
          <td>192×192×4</td>
          <td>20-40 ms</td>
          <td>87.4%</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>LSTM (Audio)</td>
          <td>1×8×40 (MFCC)</td>
          <td>30-50 ms</td>
          <td>86.3%</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>Pose Estimation</td>
          <td>-</td>
          <td>250-350 ms</td>
          <td>-</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>DST Fusion</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>50-60 us</td>
          <td>Under evaluation</td>
        </tr>
      </tbody>
    </table>
    <p>This table demonstrates a multimodal system that combines CNN and LSTM for action analysis and pose estimation, leveraging DST fusion to further enhance overall performance. The system uses different input formats (image and audio features) and is designed for robust, real-time operation in diverse environments.</p>
  </section>

  <section id="future-improvements">
    <h2>Future Improvements</h2>
    <ul>
      <li>Convert ONNX models to TensorRT to further accelerate inference speed and optimize deployment on embedded devices.</li>
      <li>Transition from using public datasets to collecting and training on custom datasets for improved real-world relevance and accuracy.</li>
      <li>Integrate a notification system to provide instant alerts to caregivers or users in the event of a detected fall.</li>
      <li>Conduct real-world validation and benchmarking of both single-model and multimodal system accuracy to ensure robust performance in practical scenarios.</li>
    </ul>
  </section>

  <footer id="contact">
    <p>Developed by Chou, Ming-kun | NCKU Cross-College Elite Program</p>
    <p>Email: AN4096750@gs.ncku.edu.tw</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
