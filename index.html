<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MK love you forever</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Real-Time Multimodal Fall Detection via Audio-Visual Fusion and DST on Edge Devices</h1>
    <p> MK Chou</p>
  </header>

  <nav>
    <ul>
      <li><a href="#overview">Overview</a></li>
      <li><a href="#motivation">Motivation</a></li>
      <li><a href="#dst-reason">DST</a></li>
      <li><a href="#system-architecture">System Architecture</a></li>
      <li><a href="#visual-component">Visual</a></li>
      <li><a href="#audio-component">Audio</a></li>
      <li><a href="#fusion-decision">Fusion</a></li>
      <li><a href="#cli-dashboard">CLI</a></li>
      <li><a href="#performance">Performance</a></li>
      <li><a href="#contact">Contact</a></li>
    </ul>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      FallFusion is a real-time, multimodal fall detection system that integrates both visual and audio streams. Leveraging a custom-trained CNN for action recognition and an LSTM for fall sound detection, the system employs Dempster-Shafer Theory (DST) for robust decision-level fusion. Optimized for the resource-constrained Jetson Nano 2GB, FallFusion features efficient memory management, seamless ONNX model deployment, and reliable GPIO hardware control, collectively enhancing system stability and practical deployment in real-world scenarios.
    </p>
  </section>

  <section id="image-section" style="text-align:center;">
    <h2 style="text-align:left;">Links</h2>
    <div class="qrcode-list">
      <div class="qrcode-row">
        <img src="Q1.png" alt="Q1 image" class="qrcode-img">
        <span class="qrcode-label">DST</span>
        <a href="https://github.com/MKChou/DST-FallNet" target="_blank" style="margin-left:12px;">GitHub Repo 1</a>
      </div>
      <div class="qrcode-row">
        <img src="Q2.png" alt="Q2 image" class="qrcode-img">
        <span class="qrcode-label">CNN</span>
        <a href="https://github.com/MKChou/fall-detection-cnn" target="_blank" style="margin-left:12px;">GitHub Repo 2</a>
      </div>
      <div class="qrcode-row">
        <img src="Q3.png" alt="Q3 image" class="qrcode-img">
        <span class="qrcode-label">LSTM</span>
        <a href="https://github.com/MKChou/fall-sound-detection-lstm" target="_blank" style="margin-left:12px;">GitHub Repo 3</a>
      </div>
    </div>
  </section>

  <section id="motivation">
    <h2>Motivation and Challenges</h2>
    <p>With an aging global population, falls have emerged as a leading cause of injury and mortality among the elderly. Conventional detection systems often depend on a single modality—such as cameras or microphones—which presents several limitations:</p>
    <ul>
      <li>Single-modality accuracy is limited: Visual data is affected by lighting and occlusion, while audio is susceptible to background noise, leading to frequent false alarms or missed detections.</li>
      <li>High resource requirements: Most deep learning systems rely on high-performance computers and cloud services, making them unsuitable for resource-constrained environments.</li>
      <li>Privacy and latency issues: Uploading images to the cloud for analysis raises privacy concerns and cannot provide real-time responses.</li>
    </ul>
    <p>Therefore, we aim to design a system that can:</p>
    <ul>
      <li>Integrate multiple sensing data (visual + audio) to improve recognition accuracy.</li>
      <li>Deploy on low-memory, low-power embedded platforms such as Jetson Nano 2GB.</li>
      <li>Support real-time response and hardware interaction (LED, buzzer) to enhance alert efficiency and practical value.</li>
    </ul>
  </section>

  <section id="dst-reason">
    <h2>Why Dempster-Shafer Theory (DST)?</h2>
    <p>Conventional multimodal fusion techniques, such as direct score averaging, are often inflexible when dealing with uncertainty and conflicting evidence. For instance:</p>
    <ul>
      <li>The visual model may have low confidence under poor lighting.</li>
      <li>The audio model may misclassify due to background noise.</li>
    </ul>
    <p>Directly combining scores in such cases may reduce system accuracy.</p>
    <p><b>How DST addresses these issues:</b></p>
    <ul>
      <li><b>Handling uncertainty:</b><br>DST enables explicit assignment of belief to "uncertainty" (e.g., m(unknown)), offering greater adaptability than traditional probabilistic frameworks.</li>
      <li><b>Fusing conflicting information:</b><br>When the visual and audio models disagree (e.g., one detects a fall, the other normal), DST calculates a "conflict coefficient K" and reallocates trust to the more reliable modality.</li>
      <li><b>Dynamic weight adjustment:</b><br>No need to manually set the weight of visual or audio; DST automatically determines which is more trustworthy based on the current input.</li>
    </ul>
  </section>

  <section id="system-architecture">
    <h2>System Architecture</h2>
    <img src="system.png" alt="System Architecture Diagram" style="max-width:90%;height:auto;display:block;margin:0 auto;">
    <p style="text-align:center;">System architecture flowchart</p>
    <p>The system synergistically integrates visual and audio modalities to achieve robust fall detection, even on low-resource embedded hardware. Its core components include:</p>
    <ul>
      <li>Pose estimation (MoveNet)</li>
      <li>Audio recognition (MFCC + LSTM)</li>
      <li>Fusion using Dempster-Shafer Theory (DST)</li>
      <li>Action verification with a custom 5-class CNN</li>
      <li>Deployment on Jetson Nano 2GB via ONNX inference</li>
    </ul>
    <h3 style="text-align:left;">The system consists of four main threads:</h3>
    <ol style="text-align:left;">
      <li><b>Visual Pose Inference Thread</b><br>
        Captures camera images, resizes and preprocesses them for input to the MoveNet ONNX model.<br>
        Infers 17 human keypoints and confidence scores.<br>
        Calculates body tilt angle to estimate suspicious postures (e.g., lying, falling).
      </li>
      <li><b>Audio Recognition Thread</b><br>
        Continuously records audio (3 seconds) via microphone.<br>
        Converts audio to MFCC (Mel Frequency Cepstral Coefficients) feature maps.<br>
        Inputs to LSTM audio model (ONNX), outputs a fall sound score (0~1).
      </li>
      <li><b>Fusion & CNN Verification Thread</b><br>
        Fuses visual and audio inference scores using DST (Dempster-Shafer Theory).<br>
        If the fusion result exceeds the threshold, captures the current image and uses a 5-class CNN model (standing, sitting, lying, bending, crawling) for final confirmation.<br>
        Updates results on the CLI Dashboard and determines whether to trigger GPIO output.
      </li>
      <li><b>GPIO Control & Button Monitoring Thread</b><br>
        Controls external hardware outputs such as LED/buzzer.<br>
        Detects button triggers, clears fall event records, and resets system status.
      </li>
    </ol>
  </section>

  <section id="visual-component">
    <h2>Visual Component</h2>
    <h3>1. MoveNet Pose Estimation</h3>
    <p>MoveNet, developed by Google, is a high-performance human pose estimation model renowned for its rapid inference and high accuracy. Its lightweight design is ideal for deployment on embedded devices such as Jetson Nano, enabling stable and accurate pose analysis even under resource constraints. With MoveNet, the system can instantly capture human motion changes, providing a solid foundation for subsequent fall detection and multimodal fusion.</p>
    <img src="movenet.gif" alt="MoveNet pose estimation demo" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">MoveNet real-time keypoint detection demo</p>
    <img src="movenet2.gif" alt="MoveNet fall detection process demo" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">MoveNet applied to fall detection process animation</p>
    <p>The system applies the MoveNet ONNX model for human pose estimation, detecting 17 keypoints (e.g., head, shoulders, elbows, knees) and outputting coordinates and confidence scores. These features are used to calculate body tilt angle and center of gravity, serving as the initial basis for fall detection. The process ensures high performance and low latency, suitable for embedded platforms.</p>
    <p>To further optimize deployment, MoveNet is quantized to int8 and converted from tflite to onnx format. This significantly reduces model size and computational requirements with minimal impact on inference accuracy, enabling stable operation on resource-limited devices like Jetson Nano while maintaining real-time and precision.</p>
    <h3>2. Custom CNN Action Classification Verification</h3>
    <p>When the DST fusion score surpasses a predefined threshold, a custom-trained CNN is activated for detailed action classification. This model, specifically tailored for the system, distinguishes among five human actions: standing, sitting, lying, bending, and crawling, each accompanied by a confidence score. If any action's confidence exceeds the alert threshold, the system promptly issues an alarm. This layered approach significantly reduces false positives and enhances the reliability of fall detection.</p>
    <p>The training dataset used is from <a href="https://falldataset.com/" target="_blank">Fall Detection Dataset</a>. Each folder contains sequential action images with a CSV file labeling the action type for each image. A total of 15,402 images were processed, split into 80% training and 20% validation sets. During training, both training and validation accuracy and loss are displayed in real time to monitor model learning and generalization.</p>
    <img src="CNN_confusion_matrix.png" alt="CNN confusion matrix" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">CNN action classification confusion matrix, showing prediction vs. actual distribution</p>
    <img src="CNN_acc_curve.png" alt="CNN accuracy curve" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">CNN training and validation accuracy curves</p>
  </section>

  <section id="audio-component">
    <h2>Audio Component</h2>
    <h3>1. Audio Data Collection and Preprocessing</h3>
    <p>The audio module continuously acquires ambient sounds via a microphone, segmenting each recording into three-second samples. Post-processing steps—including noise reduction and volume normalization—are applied before converting the waveform into Mel Frequency Cepstral Coefficient (MFCC) features for model input.</p>
    <img src="Audio_wave.png" alt="Audio waveform" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">Original audio waveform</p>
    <img src="Audio_MFCC.png" alt="Audio MFCC feature" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">Audio waveform converted to MFCC feature for model input</p>

    <h3>2. Dataset Source</h3>
    <p>The audio model is trained using the <a href="https://www.kaggle.com/datasets/antonygarciag/fall-audio-detection-dataset/data" target="_blank">AFE: Sound Analysis for Fall Event detection</a> dataset, which contains various fall events and daily environmental sounds, with each audio segment labeled as fall-related or not. These annotations help the model learn to distinguish fall sounds from general noise, improving recognition accuracy.</p>

    <h3>3. LSTM Model Training and Performance</h3>
    <ul>
      <li><b>Model architecture:</b> LSTM(64) → Dropout(0.5) → Dense(32, relu) → Dense(1, sigmoid)</li>
      <li><b>Loss function:</b> binary_crossentropy</li>
      <li><b>Optimizer:</b> Adam</li>
      <li><b>Epochs:</b> 30</li>
      <li><b>Batch size:</b> 16</li>
      <li><b>Train/Test Split:</b> 80/20</li>
    </ul>
    <p>MFCC features are used as input to train the LSTM model for fall sound recognition. The following figures show the LSTM training loss/accuracy curves, test set confusion matrix, and ROC curve, demonstrating the model's excellent performance in classifying fall and non-fall events.</p>
    <img src="Audio_LSTM_training_results.png" alt="LSTM training loss/accuracy" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM training loss and accuracy curves</p>
    <img src="Audio_Confusion_Matrix.png" alt="LSTM confusion matrix" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM model confusion matrix on the test set</p>
    <img src="Audio_ROCCurve.png" alt="LSTM ROC curve" style="max-width:90%;height:auto;display:block;margin:0.5em auto;">
    <p style="text-align:center;">LSTM model ROC curve, AUC indicates excellent classification ability</p>

    <h3>4. Advanced Evaluation Metrics</h3>
    <ul>
      <li><b>Confusion Matrix:</b> [[91, 2], [3, 94]]</li>
      <li><b>Accuracy:</b> 0.97</li>
      <li><b>Precision:</b> 0.9792</li>
      <li><b>Recall:</b> 0.9691</li>
      <li><b>F1-score:</b> 0.9741</li>
      <li><b>AUC Score:</b> 0.9973</li>
      <li><b>ROC curve:</b> Nearly perfect (see above)</li>
    </ul>

    <h3>5. Practical Application Plan</h3>
    <ul>
      <li>Real-time audio collection with recording equipment</li>
      <li>Convert audio to MFCC (40 dimensions, 3 seconds, 22050Hz)</li>
      <li>Input to the trained model (.tflite/.onnx) to determine if a fall event has occurred</li>
      <li>Output a "fall probability score" for use in multimodal fusion (combined with video, accelerometer, etc.)</li>
    </ul>
  </section>

  <section id="fusion-decision">
    <h2>Fusion & Decision</h2>
    <h3>1. Fusion Mechanism</h3>
    <p>At the heart of the system's multimodal decision-making lies Dempster-Shafer Theory (DST). Inference results from the visual (pose angles, CNN scores) and audio (LSTM scores) modules are first transformed into belief assignments. The DST fusion algorithm then calculates a unified confidence score and a conflict coefficient (K), quantifying the degree of disagreement between modalities.</p>
    <p>Specifically, this system utilizes a late fusion (decision-level fusion) strategy. Instead of directly combining raw sensor data or features, each modality (vision and audio) independently produces its own decision or confidence score. These scores are then fused at the decision level using DST. This approach allows each model to fully leverage its strengths and makes the overall system more robust to noise or errors in any single modality. Late fusion is especially suitable for scenarios where sensor characteristics and data types differ significantly, as in this fall detection application.</p>
    <p>The system is also designed to flexibly support different sensors and threshold settings for various environments. Depending on the deployment scenario, users can select the most appropriate combination of sensors (e.g., only vision, only audio, or both) and adjust the fusion or alert thresholds accordingly. This adaptability ensures optimal performance and reliability whether in a quiet home, a noisy public space, or other specialized settings.</p>

    <h3>2. Threshold & Trigger</h3>
    <p>When the DST fused confidence score exceeds a preset threshold, the system further activates the CNN model for multi-class action verification. If the CNN determines a high-risk action (such as lying or crawling) and the confidence exceeds the alert threshold, an alert signal (e.g., buzzer, LED) is triggered and the event is logged in the CLI Dashboard.</p>

    <h3>3. Conflict Handling</h3>
    <p>One major advantage of DST is its ability to dynamically handle conflicts between modalities. For example, when the visual and audio modules produce opposite results, DST calculates the conflict coefficient K and automatically adjusts the weights, assigning trust to the more reliable source. This prevents system failure due to misjudgment by a single modality.</p>

    <h3>4. Practical Value</h3>
    <p>DST-based fusion and decision-making significantly enhance the system's robustness and accuracy. Even when a single sensor is disturbed (e.g., poor lighting, environmental noise), the system can still make stable judgments based on multimodal information, reducing false alarms and missed detections. This makes it suitable for long-term automated monitoring applications.</p>

  
  </section>

  <section id="cli-dashboard">
    <h2>CLI Dashboard & Monitoring</h2>
    <h3>1. Interface Design</h3>
    <p>The CLI Dashboard adopts a streamlined, text-based interface, presenting information in well-organized tables and blocks. Color coding (e.g., red for alerts, green for normal) facilitates rapid status recognition. Designed for clarity and efficiency, the interface is ideal for embedded devices and remote terminal access.</p>

    <h3>2. Real-Time Monitoring Features</h3>
    <p>The CLI interface displays multiple key pieces of information in real time, including:
      <ul>
        <li>System runtime status (memory, CPU usage, latency, etc.)</li>
        <li>MoveNet pose keypoint coordinates and confidence values</li>
        <li>Audio MFCC features and LSTM scores</li>
        <li>DST fusion score and conflict coefficient</li>
        <li>CNN multi-class results and confidence values</li>
        <li>Current alert status (e.g., fall detected, alert triggered)</li>
      </ul>
    </p>

    <h3>3. Event Log and Alert Confirmation</h3>
    <p>The lower part of the interface records all fall events with time, confidence score, and action type for easy query and tracking. When an alert is triggered, it is clearly marked, and users can confirm or reset events directly via the CLI.</p>

    <h3>4. Cross-Platform and Lightweight</h3>
    <p>The CLI Dashboard runs on various operating systems and supports remote SSH connections, making it ideal for embedded platforms (such as Jetson Nano) or server-side deployment. Its lightweight nature ensures smooth operation even in resource-constrained environments.</p>

    <h3>5. Practical Value</h3>
    <p>The CLI interface enables on-site personnel or remote maintenance engineers to instantly grasp system status, quickly respond to abnormal events, and serves as a foundation for long-term monitoring and data analysis. It is especially valuable in environments where graphical interfaces are unavailable (e.g., factories, edge devices).</p>

    <img src="CLI.png" alt="CLI Dashboard interface" style="max-width:90%;height:auto;display:block;margin:0 auto;">
    <p style="text-align:center;">CLI Dashboard monitoring interface demo</p>
  </section>

  <section id="performance">
    <h2>Model Performance</h2>
    <p>The following table provides a comprehensive summary of the multimodal system, highlighting the integration of CNN and LSTM for action analysis and pose estimation, with DST fusion further enhancing overall performance.</p>
    <table>
      <thead>
        <tr>
          <th>Model</th>
          <th>Input Shape</th>
          <th>Inference Latency</th>
          <th>Accuracy</th>
          <th>DST Fusion Latency</th>
          <th>Status</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>CNN (5-class)</td>
          <td>192×192×4</td>
          <td>20-40 ms</td>
          <td>87.4%</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>LSTM (Audio)</td>
          <td>1×8×40 (MFCC)</td>
          <td>30-50 ms</td>
          <td>86.3%</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>Pose Estimation</td>
          <td>-</td>
          <td>250-350 ms</td>
          <td>-</td>
          <td>-</td>
          <td>Deployed</td>
        </tr>
        <tr>
          <td>DST Fusion</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>50-60 us</td>
          <td>Under evaluation</td>
        </tr>
      </tbody>
    </table>
    <p>This table demonstrates a multimodal system that combines CNN and LSTM for action analysis and pose estimation, leveraging DST fusion to further enhance overall performance. The system uses different input formats (image and audio features) and is designed for robust, real-time operation in diverse environments.</p>
  </section>

  <section id="future-improvements">
    <h2>Future Improvements</h2>
    <ul>
      <li>Convert ONNX models to TensorRT to further accelerate inference speed and optimize deployment on embedded devices.</li>
      <li>Transition from public datasets to the collection and training of custom data, with the goal of achieving higher real-world relevance and improved accuracy.</li>
      <li>Integrate a notification system to provide instant alerts to caregivers or users in the event of a detected fall.</li>
      <li>Conduct real-world validation and benchmarking of both single-model and multimodal system accuracy to ensure robust performance in practical scenarios.</li>
    </ul>
  </section>

  <footer id="contact">
    <p>Developed by Chou, Ming-kun | NCKU Cross-College Elite Program</p>
    <p>Email: AN4096750@gs.ncku.edu.tw</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
