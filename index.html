<!-- index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>MK love you forever
  </title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Real-Time Multimodal Fall Detection via Audio-Visual Fusion and DST on Edge Devices
    </h1>
    <p> MK Chou</p>
  </header>

  <nav>
    <ul>
      <li><a href="#overview">Overview</a></li>
      <li><a href="#features">Features</a></li>
      <li><a href="#performance">Performance</a></li>
      <li><a href="#contact">Contact</a></li>
    </ul>
  </nav>

  <section id="overview">
    <h2>Overview</h2>
    <p>
      FallFusion is a real-time multimodal fall detection system that integrates visual and audio sensing data. It employs a self-trained CNN model for action classification and an LSTM model for fall sound recognition, applying Dempster-Shafer Theory for decision-level fusion. The system has been successfully deployed on the resource-constrained Jetson Nano 2GB platform, demonstrating efficient memory usage, ONNX model cross-platform integration, and GPIO hardware control capabilities, thereby enhancing overall stability and application feasibility.
    </p>
  </section>

  <section id="image-section" style="text-align:center;">
    <h2 style="text-align:left;">Links </h2>
    <div class="qrcode-list">
      <div class="qrcode-row">
        <img src="Q1.png" alt="Q1 圖片" class="qrcode-img">
        <span class="qrcode-label">DST</span>
      </div>
      <div class="qrcode-row">
        <img src="Q2.png" alt="Q2 圖片" class="qrcode-img">
        <span class="qrcode-label">CNN</span>
      </div>
      <div class="qrcode-row">
        <img src="Q3.png" alt="Q3 圖片" class="qrcode-img">
        <span class="qrcode-label">LSTM</span>
      </div>
    </div>
  </section>

  <section id="motivation">
    <h2>研究動機與問題點</h2>
    <p>隨著高齡化社會來臨，跌倒事件已成為老年人傷亡的主要原因之一。傳統的偵測系統多依賴單一模態（如攝影機或麥克風），但：</p>
    <ul>
      <li>單一模態準確率受限：視覺受光影遮蔽影響，音訊易受背景噪音干擾，單獨判斷常出現誤報或漏報。</li>
      <li>高資源門檻：大多數深度學習系統仰賴高效能電腦與雲端服務，無法適用於資源受限環境。</li>
      <li>隱私與延遲問題：影像上傳雲端進行判斷易引發隱私疑慮，且無法提供即時回應。</li>
    </ul>
    <p>因此，我們希望設計一套能：</p>
    <ul>
      <li>整合多種感測資料（視覺＋音訊），提升辨識精度。</li>
      <li>部署於低記憶體、低耗電的嵌入式平台，如 Jetson Nano 2GB。</li>
      <li>支援即時反應與硬體互動（LED、蜂鳴器），提升警示效率與真實應用價值。</li>
    </ul>
  </section>

  <section id="dst-reason">
    <h2>為何選用 Dempster-Shafer Theory（DST）？</h2>
    <p>傳統多模態融合（例如直接平均分數）在處理不確定性與衝突時缺乏彈性。例如：</p>
    <ul>
      <li>視覺模型在光源不足時可能信心值低。</li>
      <li>音訊模型在背景噪音中可能出現誤判。</li>
    </ul>
    <p>這時若直接合併兩者分數，反而可能降低系統準確性。</p>
    <p><b>DST 解決痛點的方式：</b></p>
    <ul>
      <li><b>處理不確定性：</b><br>DST 可針對「不確定」本身給予一個信任分配（例如 m(未知)），比傳統機率方法更有彈性。</li>
      <li><b>融合衝突資訊：</b><br>當視覺模型與音訊模型意見不同時（如一個判斷跌倒、一個判斷正常），DST 可計算「衝突係數 K」，並重新分配信任給更可靠的模態。</li>
      <li><b>動態權重調整：</b><br>不需手動設定視覺或音訊的比重，而是依照當下輸入資料自動決定誰更可信。</li>
    </ul>
  </section>

  <section id="system-architecture">
    <h2>系統架構</h2>
    <img src="system.png" alt="系統架構圖" style="max-width:90%;height:auto;display:block;margin:0 auto;">
    <p style="text-align:center;">系統架構流程圖</p>
    <p>This system combines vision and sound to detect falls more robustly, all while running on a low-resource embedded device. The core components include:</p>
    <ul>
      <li>Pose estimation (MoveNet)</li>
      <li>Audio recognition (MFCC + LSTM)</li>
      <li>Fusion via Dempster-Shafer Theory (DST)</li>
      <li>Confirmation using a self-trained 5-class CNN</li>
      <li>Deployment on Jetson Nano 2GB using ONNX inference</li>
    </ul>

    <h3 style="text-align:left;">系統分為四個主要執行緒：</h3>
    <ol style="text-align:left;">
      <li><b>影像姿態推論執行緒（Visual Thread）</b><br>
        擷取攝影機影像，經過 Resize 與轉換後輸入至 MoveNet ONNX 模型<br>
        推論出 17 個人體關鍵點位置與置信度<br>
        計算身體傾斜角度，推估是否為可疑姿勢（如躺、跌倒）
      </li>
      <li><b>音訊辨識執行緒（Audio Thread）</b><br>
        使用麥克風實時錄音（3 秒）<br>
        將音訊轉為 MFCC（Mel Frequency Cepstral Coefficients）特徵圖<br>
        輸入至 LSTM 音訊模型（ONNX），輸出是否為跌倒聲音的分數（0~1）
      </li>
      <li><b>決策融合與 CNN 驗證執行緒（Fusion & CNN Thread）</b><br>
        根據視覺與音訊推論分數，使用 DST（Dempster-Shafer Theory） 計算融合信心值<br>
        若融合結果高於門檻，擷取當下影像，使用 CNN 多分類模型（5 類：站、坐、躺、彎、爬）進行最終確認<br>
        結果更新至 CLI Dashboard，並決定是否觸發 GPIO 輸出
      </li>
      <li><b>GPIO 控制與按鍵監聽執行緒（GPIO Thread）</b><br>
        控制 LED / 蜂鳴器 等外部硬體輸出<br>
        偵測按鈕觸發，清除跌倒事件紀錄並重置系統狀態
      </li>
    </ol>
  </section>

  <section id="performance">
    <h2>Model Performance</h2>
    <table>
      <thead>
        <tr><th>Model</th><th>Accuracy</th><th>Size</th><th>Input</th></tr>
      </thead>
      <tbody>
        <tr><td>CNN (5-class)</td><td>89.2%</td><td>~3.2MB</td><td>192×192×3</td></tr>
        <tr><td>LSTM (Audio)</td><td>97%</td><td>~1.1MB</td><td>MFCC (130×40)</td></tr>
      </tbody>
    </table>
  </section>

  <footer id="contact">
    <p>Developed by 周明坤 Chou, Ming-kun | NCKU Cross-College Elite Program</p>
    <p>Email: AN4096750@gs.ncku.edu.tw</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
